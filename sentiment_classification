
Sentiment Classification


Created by Nathan Ewing, Sung Chol Hong, and Dong Yun Kim

Install and import modules

# ! pip install --user gender-guesser nltk textblob
# ! python -m textblob.download_corpora
# ! pip install --user --upgrade scikit-learn
# ! pip install --user --upgrade scikit-learn pyldavis

import gender_guesser.detector as gender
import nltk
from textblob import TextBlob
import gender_guesser.detector as gender

Import CSV file

import pandas as pd
pd.set_option('display.max_colwidth', 150)

df = pd.read_csv("html_metadata.csv", sep = "\t")
df


Dataframe information

df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3270 entries, 0 to 3269
Data columns (total 5 columns):
 #   Column               Non-Null Count  Dtype 
---  ------               --------------  ----- 
 0   file_name            3270 non-null   object
 1   article_title        3270 non-null   object
 2   article_author       1928 non-null   object
 3   posting_dateandtime  3270 non-null   object
 4   article_text         3263 non-null   object
dtypes: object(5)
memory usage: 127.9+ KB


Concatenate article_text

sent_list = df.article_text.str.cat()

Tokenize article_text into sentences

import nltk
sent_list = nltk.sent_tokenize(sent_list)

Extract 2000 random tokenized sentences

import random
randomSent = random.sample(sent_list, 2000)
randomSent


Apply polarity score to each random sentence

import random
import pandas as pd

random2000sent = random.sample(sent_list, 2000)
dftmp1 = pd.DataFrame(random2000sent)
dftmp1 = dftmp1.rename(columns = {0:'random_sentences'})
dftmp1["polarity_score"] = dftmp1.random_sentences.apply(lambda x: TextBlob(x).sentiment.polarity)


def predict_polarity(score):
    if (score > -0.2) & (score < 0.2):
        return "neutral"
    if score <= -0.2:
        return "negative"
    if score >= 0.2:
        return "positive"
    
dftmp1["sentence_polarity"] = dftmp1.polarity_score.apply(lambda x: predict_polarity(x))
dftmp1

random_sentences	polarity_score	sentence_polarity

        0	He’s accustomed to going to the polls on Election Day.	0.000000	neutral
        1	“From basic lack of access to health care, transportation, and protections in the workplace, these inequities hit people of color and vulnerable c...	-0.121212	neutral
        2	“If anyone thinks that simply easing restrictions currently in place will lead to an immediate return to normal, they need to think again.	0.050000	neutral
        3	They’re intended to tell who had previously been infected whether they knew it or not.ADADThe New York study is the latest in a small wave of earl...	0.147712	neutral
        4	No need to wear a mask.	0.000000	neutral
        ...	...	...	...
        1995	In the center of Monterrey, Mexico’s third-largest city, the Sastrería Garibaldi has for 30 years made mariachi outfits.	-0.100000	neutral
        1996	Weeks after the World Health Organization declared covid-19 a pandemic, states still faced shortages of the equipment necessary to fight the virus.	0.000000	neutral
        1997	Or we could be strategic — identifying the sick and at-risk through testing and tracing, suppressing outbreaks, building up our public health capa...	-0.238095	negative
        1998	For sure, some will, just as pockets of resistance to the New Deal remained entrenched for decades.	0.318182	positive
        1999	Meanwhile, it’s causing turmoil in the global economy, financial markets and even the world of sports.	0.000000	neutral
        2000 rows × 3 columns

Save as CSV file

import pandas as pd
dftmp1.to_csv("polarity_classification.csv") # This file with polarity score.

Import manually classified CSV file

import pandas as pd
df2 = pd.read_csv("random_sent_classification.csv") # We went through each sentence and applied classification.
df2


            random_sentences	sentiment_classification
            0	And we need our government to mobilize.	2
            1	â€œAt our main treatment center, almost nothin...	2
            2	He took to talk radio and â€œFox & Friendsâ€ ...	0
            3	Most of America is Andy Carvin ðŸ‘‡ðŸ» https:...	3
            4	All rights reserved.	2
            ...	...	...
            1995	He is attracted to Anneâ€™s service because â€...	2
            1996	We continue to refine our models and assumptio...	2
            1997	Then he realized he was exempt.	2
            1998	After a decade of planning, postcards will sta...	2
            1999	If 2 million people sign up and get all three ...	2
            2000 rows × 2 columns

Create a function to determine class

def predict_class_type(s_type):
    if s_type == 0:
        return "Negative"
    if s_type == 1:
        return "Positive"
    if s_type == 2:
        return "Neutral"
    if s_type == 3:
        return "None"
    
df2["sent_class_type"] = df2.sentiment_classification.apply(lambda x: predict_class_type(x))
df2


              random_sentences	sentiment_classification	sent_class_type
              0	And we need our government to mobilize.	2	Neutral
              1	â€œAt our main treatment center, almost nothin...	2	Neutral
              2	He took to talk radio and â€œFox & Friendsâ€ ...	0	Negative
              3	Most of America is Andy Carvin ðŸ‘‡ðŸ» https:...	3	None
              4	All rights reserved.	2	Neutral
              ...	...	...	...
              1995	He is attracted to Anneâ€™s service because â€...	2	Neutral
              1996	We continue to refine our models and assumptio...	2	Neutral
              1997	Then he realized he was exempt.	2	Neutral
              1998	After a decade of planning, postcards will sta...	2	Neutral
              1999	If 2 million people sign up and get all three ...	2	Neutral
              2000 rows × 3 columns




Test using models



1. K - Nearest Neighbor Algorithm
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, norm="l2", stop_words="english", max_df=0.7)
X = vectorizer.fit_transform(df2.random_sentences)
y = df2.sentiment_classification

X.shape, y.shape
((2000, 8370), (2000,))
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
from sklearn.neighbors import KNeighborsClassifier 
knn = KNeighborsClassifier(n_neighbors=1)  

knn.fit(x_train, y_train)
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,
                     weights='uniform')
knn.score(x_train, y_train), knn.score(x_test, y_test)
(0.9825, 0.625)
pred = knn.predict(x_test)

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, pred))
              precision    recall  f1-score   support

           0       0.33      0.01      0.02        84
           1       0.00      0.00      0.00        52
           2       0.63      0.98      0.76       248
           3       0.70      0.44      0.54        16

    accuracy                           0.62       400
   macro avg       0.42      0.36      0.33       400
weighted avg       0.49      0.62      0.50       400

print(confusion_matrix(y_test,pred))
[[  1   0  83   0]
 [  0   0  52   0]
 [  2   1 242   3]
 [  0   0   9   7]]
from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=1)
scores_k1 = cross_val_score(knn, x_train, y_train, cv=5)
scores_k1
array([0.234375, 0.621875, 0.625   , 0.628125, 0.615625])
scores_k1.mean(), scores_k1.std()
(0.545, 0.15536750786441803)
knn = KNeighborsClassifier(n_neighbors=3)
scores_k3 = cross_val_score(knn, x_train, y_train, cv=5)
scores_k3
array([0.2375  , 0.228125, 0.215625, 0.2125  , 0.228125])
scores_k3.mean(), scores_k3.std()
(0.224375, 0.009142961773954867)
score_max = 0                      # Score_max is a temoporay variable to store the max score 
for param in [1, 2, 3, 4, 5]:
    model = KNeighborsClassifier(n_neighbors=param)
    scores = cross_val_score(model, x_train, y_train, cv=5)
    print("k = {}: {}\n{:.3f}, {:.3f}\n".format(param, scores, scores.mean(), scores.std()))
    
    if scores.mean() > score_max:
        score_max = scores.mean()
        param_best = param         # Param_best is a temoporay variable to store the best parameter 
        
print("Highest score : {:.3f} when k = {}".format(score_max, param_best))
k = 1: [0.234375 0.621875 0.625    0.628125 0.615625]
0.545, 0.155

k = 2: [0.221875 0.621875 0.634375 0.628125 0.625   ]
0.546, 0.162

k = 3: [0.2375   0.228125 0.215625 0.2125   0.228125]
0.224, 0.009

k = 4: [0.225    0.165625 0.25625  0.125    0.29375 ]
0.213, 0.061

k = 5: [0.24375  0.215625 0.265625 0.146875 0.2875  ]
0.232, 0.049

Highest score : 0.546 when k = 2
def train_test(x_train, x_test, y_train, y_test, classifier, ismultilabel=False):
    classifier.fit(x_train, y_train)
    pred = classifier.predict(x_test)
    
    print("Train score: {:.2f}".format(classifier.score(x_train, y_train)))
    print("Test score: {:.2f}\n".format(classifier.score(x_test, y_test)))
    print("Classification report:\n{}".format(classification_report(y_test, pred, zero_division=0)))
    
    if not ismultilabel:
        print(confusion_matrix(y_test,pred))
    
    return classifier



print("k = {}".format(param_best))
knn = KNeighborsClassifier(n_neighbors=param_best)
knn = train_test(x_train, x_test, y_train, y_test, knn)
k = 2
Train score: 0.95
Test score: 0.62

Classification report:
              precision    recall  f1-score   support

           0       0.33      0.01      0.02        84
           1       0.00      0.00      0.00        52
           2       0.62      0.98      0.76       248
           3       1.00      0.25      0.40        16

    accuracy                           0.62       400
   macro avg       0.49      0.31      0.30       400
weighted avg       0.50      0.62      0.49       400

[[  1   0  83   0]
 [  0   0  52   0]
 [  2   2 244   0]
 [  0   0  12   4]]
summary = {}
summary["k-NNs"] = round(knn.score(x_test, y_test), 3)

summary
{'k-NNs': 0.622}
1.1 Predictions on unseen data (Abstracted from New York Times news articles)
text1 = "That has enabled them to leap ahead to the next step and schedule tests involving more than 6,000 people by the end of next month, hoping to show that their vaccine not only is safe, but also works."
text2 = "But social distancing is not coming naturally to every rugby player."
text3 = "The Chinese government fired right back."
text4 = "Haney was accused of verbal abuse and mistreatment of athletes, which included forcing them to train through injuries.."
text5 = "Sports get postponed, sports get canceled, and, yes, sometimes, someday, somewhere, somehow, sports can resume."
new_texts = [text1, text2, text3, text4, text5]
x_new = vectorizer.transform(new_texts)

knn.predict(x_new)
array([2, 2, 1, 2, 2])
2. Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(solver="lbfgs")
scores = cross_val_score(lr, x_train, y_train, cv=5)
print("{}\n{:.3f}, {:.3f}".format(scores, scores.mean(), scores.std()))
[0.621875 0.621875 0.6375   0.628125 0.628125]
0.628, 0.006
lr = train_test(x_train, x_test, y_train, y_test, lr)
Train score: 0.70
Test score: 0.64

Classification report:
              precision    recall  f1-score   support

           0       0.67      0.02      0.05        84
           1       1.00      0.04      0.07        52
           2       0.63      1.00      0.77       248
           3       1.00      0.25      0.40        16

    accuracy                           0.64       400
   macro avg       0.82      0.33      0.32       400
weighted avg       0.70      0.64      0.51       400

[[  2   0  82   0]
 [  0   2  50   0]
 [  1   0 247   0]
 [  0   0  12   4]]
2.1 Predictions on unseen data (Abstracted from New York Times news articles)
summary["Logistic Regression"] = round(lr.score(x_test, y_test), 3)

lr.predict(x_new)
array([2, 2, 2, 2, 2])
3. Multinomial Naive Bayes
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()
scores = cross_val_score(mnb, x_train, y_train, cv=5)
print("{}\n{:.3f}, {:.3f}".format(scores, scores.mean(), scores.std()))
[0.628125 0.625    0.63125  0.63125  0.628125]
0.629, 0.002
mnb = train_test(x_train, x_test, y_train, y_test, mnb)
Train score: 0.64
Test score: 0.62

Classification report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        84
           1       0.00      0.00      0.00        52
           2       0.62      1.00      0.77       248
           3       1.00      0.12      0.22        16

    accuracy                           0.62       400
   macro avg       0.41      0.28      0.25       400
weighted avg       0.43      0.62      0.48       400

[[  0   0  84   0]
 [  0   0  52   0]
 [  0   0 248   0]
 [  0   0  14   2]]
3.1 Predictions on unseen data (Abstracted from New York Times news articles)
summary["Multinomial Naive Bayes"] = round(mnb.score(x_test, y_test), 3)
mnb.predict(x_new)
array([2, 2, 2, 2, 2])
4. Linear Support Vector Machines (SVMs)
from sklearn.svm import LinearSVC
svm = LinearSVC(C=1)
svm = train_test(x_train, x_test, y_train, y_test, svm)
Train score: 0.98
Test score: 0.63

Classification report:
              precision    recall  f1-score   support

           0       0.48      0.15      0.23        84
           1       0.38      0.06      0.10        52
           2       0.65      0.93      0.76       248
           3       0.86      0.38      0.52        16

    accuracy                           0.63       400
   macro avg       0.59      0.38      0.40       400
weighted avg       0.58      0.63      0.56       400

[[ 13   1  70   0]
 [  2   3  47   0]
 [ 12   4 231   1]
 [  0   0  10   6]]
score_max = 0
for param in [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]:
    model = LinearSVC(C=param)
    scores = cross_val_score(model, x_train, y_train, cv=5)
    print("C = {}: {}\n{:.3f}, {:.3f}\n".format(param, scores, scores.mean(), scores.std()))
    
    if scores.mean() > score_max:
        score_max = scores.mean()
        param_best = param
        
print("Highest score : {:.3f} when C = {}".format(score_max, param_best))
C = 0.01: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 0.03: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 0.1: [0.621875 0.61875  0.634375 0.63125  0.628125]
0.627, 0.006

C = 0.3: [0.63125  0.621875 0.6375   0.6375   0.63125 ]
0.632, 0.006

C = 1: [0.621875 0.609375 0.6375   0.609375 0.603125]
0.616, 0.012

C = 3: [0.559375 0.59375  0.60625  0.596875 0.5875  ]
0.589, 0.016

C = 10: [0.55     0.559375 0.6      0.5625   0.575   ]
0.569, 0.017

/home/dongykim/.local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
C = 30: [0.525    0.546875 0.575    0.553125 0.571875]
0.554, 0.018

/home/dongykim/.local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  "the number of iterations.", ConvergenceWarning)
C = 100: [0.49375  0.53125  0.571875 0.521875 0.56875 ]
0.537, 0.030

Highest score : 0.632 when C = 0.3
print("C = {}".format(param_best))
svm = LinearSVC(C=param_best)
svm = train_test(x_train, x_test, y_train, y_test, svm)
C = 0.3
Train score: 0.88
Test score: 0.64

Classification report:
              precision    recall  f1-score   support

           0       0.62      0.06      0.11        84
           1       0.67      0.04      0.07        52
           2       0.64      0.98      0.77       248
           3       1.00      0.38      0.55        16

    accuracy                           0.64       400
   macro avg       0.73      0.36      0.38       400
weighted avg       0.65      0.64      0.53       400

[[  5   0  79   0]
 [  0   2  50   0]
 [  3   1 244   0]
 [  0   0  10   6]]
4.1 Predictions on unseen data (Abstracted from New York Times news articles)
summary["Linear SVMs"] = round(svm.score(x_test, y_test), 3)

svm.predict(x_new)
array([2, 2, 2, 2, 2])
5. Kernelized Support Vector Machines (KSVMs)
from sklearn.svm import SVC
ksvm = SVC(C=1, kernel="rbf", gamma="auto")

ksvm = train_test(x_train, x_test, y_train, y_test, ksvm)
Train score: 0.63
Test score: 0.62

Classification report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        84
           1       0.00      0.00      0.00        52
           2       0.62      1.00      0.77       248
           3       0.00      0.00      0.00        16

    accuracy                           0.62       400
   macro avg       0.15      0.25      0.19       400
weighted avg       0.38      0.62      0.47       400

[[  0   0  84   0]
 [  0   0  52   0]
 [  0   0 248   0]
 [  0   0  16   0]]
core_max = 0
for param in [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]:
    model = SVC(C=param, kernel="rbf", gamma="auto")
    scores = cross_val_score(model, x_train, y_train, cv=5)
    print("C = {}: {}\n{:.3f}, {:.3f}\n".format(param, scores, scores.mean(), scores.std()))
    
    if scores.mean() > score_max:
        score_max = scores.mean()
        param_best = param
        
print("Highest score : {:.3f} when C = {}".format(score_max, param_best))
C = 0.01: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 0.03: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 0.1: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 0.3: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 1: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 3: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 10: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 30: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

C = 100: [0.628125 0.628125 0.63125  0.628125 0.628125]
0.629, 0.001

Highest score : 0.632 when C = 0.3
print("C = {}".format(param_best))
ksvm = SVC(C=param_best)
ksvm = train_test(x_train, x_test, y_train, y_test, ksvm)
C = 0.3
Train score: 0.63
Test score: 0.62

Classification report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        84
           1       0.00      0.00      0.00        52
           2       0.62      1.00      0.77       248
           3       0.00      0.00      0.00        16

    accuracy                           0.62       400
   macro avg       0.15      0.25      0.19       400
weighted avg       0.38      0.62      0.47       400

[[  0   0  84   0]
 [  0   0  52   0]
 [  0   0 248   0]
 [  0   0  16   0]]
5.1 Predictions on unseen data (Abstracted from New York Times news articles)
summary["Kernelized SVMs"] = round(ksvm.score(x_test, y_test), 3)

ksvm.predict(x_new)
array([2, 2, 2, 2, 2])
6. Neural Networks
from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(10, ), activation="relu", random_state=0)

mlp = train_test(x_train, x_test, y_train, y_test, mlp)
Train score: 0.99
Test score: 0.57

Classification report:
              precision    recall  f1-score   support

           0       0.30      0.21      0.25        84
           1       0.31      0.15      0.21        52
           2       0.64      0.79      0.71       248
           3       0.78      0.44      0.56        16

    accuracy                           0.57       400
   macro avg       0.51      0.40      0.43       400
weighted avg       0.53      0.57      0.54       400

[[ 18   3  63   0]
 [  8   8  36   0]
 [ 35  15 196   2]
 [  0   0   9   7]]
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
### Will take a while to run this cell

score_max = 0
for param in [10, 30, 100, 300, 1000]:
    model = MLPClassifier(hidden_layer_sizes=(param, ), activation="relu", random_state=0)
    scores = cross_val_score(model, x_train, y_train, cv=5)
    print("hidden_layer_size = {}: {}\n{:.3f}, {:.3f}\n".format(param, scores, scores.mean(), scores.std()))
    
    if scores.mean() > score_max:
        score_max = scores.mean()
        param_best = param
        
print("Highest score : {:.3f} when hidden_layer_sizes = {}".format(score_max, param_best))
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
hidden_layer_size = 10: [0.540625 0.53125  0.6125   0.56875  0.58125 ]
0.567, 0.029

/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
hidden_layer_size = 30: [0.54375  0.54375  0.59375  0.56875  0.578125]
0.566, 0.020

hidden_layer_size = 100: [0.5125   0.528125 0.58125  0.546875 0.56875 ]
0.548, 0.025

hidden_layer_size = 300: [0.509375 0.528125 0.58125  0.55625  0.571875]
0.549, 0.027

hidden_layer_size = 1000: [0.509375 0.534375 0.571875 0.540625 0.571875]
0.546, 0.024

Highest score : 0.567 when hidden_layer_sizes = 10
print("hidden_layer_size = {}".format(param_best))
mlp = MLPClassifier(hidden_layer_sizes=(param_best, ), random_state=0)
mlp = train_test(x_train, x_test, y_train, y_test, mlp)

summary["Neural Networks"] = round(mlp.score(x_test, y_test), 3)
hidden_layer_size = 10
Train score: 0.99
Test score: 0.57

Classification report:
              precision    recall  f1-score   support

           0       0.30      0.21      0.25        84
           1       0.31      0.15      0.21        52
           2       0.64      0.79      0.71       248
           3       0.78      0.44      0.56        16

    accuracy                           0.57       400
   macro avg       0.51      0.40      0.43       400
weighted avg       0.53      0.57      0.54       400

[[ 18   3  63   0]
 [  8   8  36   0]
 [ 35  15 196   2]
 [  0   0   9   7]]
/home/dongykim/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
6.1 Predictions on unseen data (Abstracted from New York Times news articles)
mlp.predict(x_new)
array([2, 1, 1, 2, 0])
Final Model: SVM
Testing SVM Model on 10 random sentences collected from New York Times
text1 = "It is impossible to know at this stage whether any vaccine will work, or which will be the first to emerge as a success."
text2 = "That has enabled them to leap ahead to the next step and schedule tests involving more than 6,000 people by the end of next month, hoping to show that their vaccine not only is safe, but also works."
text3 = "Nurses working under her auspices had been viciously attacked around the country at least 21 times, accused of spreading the coronavirus."
text4 = "In the Philippines, attackers doused a nurse with bleach, blinding him."
text5 = "Even as the global death toll passed 200,000, countries are mapping out a return to public life."
text6 = "The Oxford scientists now say that with an emergency approval from regulators, the first few million doses of their vaccine could be available by September — at least several months ahead of any of the other announced efforts — if it proves to be effective."
text7 = "It is impossible to know at this stage whether any vaccine will work, or which will be the first to emerge as a success."
text8 = "More than 28 days later all six were healthy, said Vincent Munster, the researcher who conducted the test."
text9 = "Even some members of Mr. Macron’s party expressed frustration with the speedy schedule."
text10 = "France has had nearly 130,000 confirmed infections of the new coronavirus and more than 23,000 deaths."
new_texts1 = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]
x_new1 = vectorizer.transform(new_texts1)

svm.predict(x_new1)
array([2, 2, 2, 2, 0, 2, 2, 2, 2, 0])
End of Part 2¶
